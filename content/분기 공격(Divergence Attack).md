분기 공격(Divergence Attack)은 [[안전성 정렬(Safety Alignment)]]된 LLM을 챗봇 모드에서 이탈시켜 기반 언어 모델(base LM)처럼 동작하게 만들어, 훈련 데이터를 추출하는 공격 기법이다. Nasr et al.(2023)이 ChatGPT에 대해 처음 대규모로 실증했다.

## 공격 원리

1. **토큰 반복 프롬프트**: `"poem poem poem poem poem..."` 처럼 단일 토큰을 수백 번 반복하는 프롬프트를 입력한다.
2. **분기(Divergence) 유도**: 반복 토큰이 모델을 RLHF로 정렬된 챗봇 모드에서 벗어나게 하여, 기반 언어 모델처럼 일반 인터넷 텍스트 스타일로 출력하게 만든다.
3. **기억된 데이터 추출**: 이 상태에서 모델은 훈련 데이터를 그대로 재생성한다.

## 주요 결과

Nasr et al.의 연구에서는 약 $200의 API 비용으로:

- **10,000개 이상**의 verbatim(원문 그대로) 훈련 데이터 예시를 추출
- 출력의 **5% 이상**이 50토큰 연속 훈련 데이터와 일치
- 추출된 데이터에는 실제 사람의 이메일 주소, 전화번호, 주소 등 **개인 식별 정보(PII)**가 포함

더 많은 비용을 투자하면 약 1GB의 훈련 데이터 추출이 가능할 것으로 추정되었다.

## 방어의 어려움

반복 토큰 공격 자체는 입력 필터링으로 쉽게 막을 수 있지만, 근본 원인인 모델의 과잉 암기(memorization)와 분기 취약성은 해결이 훨씬 어렵다. 후속 연구에서는 단일 토큰이 아닌 다중 토큰 반복 시퀀스로도 GPT-3.5와 GPT-4에서 동일한 분기 효과가 재현되었다.

이 연구는 정렬이 모델의 표면적 행동만 바꿀 뿐 내부에 기억된 훈련 데이터를 제거하지 않는다는 점에서, 안전성 정렬의 얕은 본질을 다른 각도에서 확인시켜 준다.

#### 참고 자료
1. Nasr et al. (2023), "Scalable Extraction of Training Data from (Production) Language Models", [arXiv:2311.17035](https://arxiv.org/abs/2311.17035)
2. arXiv:2412.11302 (2024), 추출률 지표의 2.14배 과소평가 발견
